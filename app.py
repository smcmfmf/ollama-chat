from flask import Flask, request, jsonify, Response, stream_with_context, render_template
import requests
import json
import os
import threading

OLLAMA_HOST = "http://host.docker.internal:11434"
OLLAMA_URL = f"{OLLAMA_HOST}/api/generate"
OLLAMA_CHAT_URL = f"{OLLAMA_HOST}/api/chat"

MEMORY_FILE = "user_memory.txt"

app = Flask(__name__)

@app.get("/")
def index():
    return render_template("index.html")

@app.get("/2")
def index2():
    return render_template("index2.html")

def get_memory():
    """ì €ì¥ëœ ì‚¬ìš©ì ê¸°ì–µì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if not os.path.exists(MEMORY_FILE):
        return ""
    try:
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            return f.read().strip()
    except Exception:
        return ""

def update_memory_task(model, user_input):
    """
    [ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…]
    ì‚¬ìš©ì ì…ë ¥ì—ì„œ 'ì¤‘ìš”í•œ ì •ë³´'ê°€ ë°œê²¬ë  ë•Œë§Œ ê¸°ì–µì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´, ê¸°ì–µë„ í•œêµ­ì–´ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
    """
    
    extraction_prompt = f"""
    ì—­í• : ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” 'ê¸°ì–µ ê´€ë¦¬ì'ì…ë‹ˆë‹¤.
    
    ì‚¬ìš©ì ì…ë ¥: "{user_input}"
    
    ì§€ì‹œì‚¬í•­:
    1. ìœ„ 'ì‚¬ìš©ì ì…ë ¥'ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤(ì´ë¦„, ì·¨ë¯¸, ì„ í˜¸ë„, ì¼ì •, íŠ¹ì§• ë“±)ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
    2. ê¸°ì–µí•  ë§Œí•œ ê°€ì¹˜ê°€ ìˆëŠ” ì •ë³´ë¼ë©´, ê·¸ ì‚¬ì‹¤ì„ ê°„ê²°í•œ 'í•œêµ­ì–´ ë¬¸ì¥'ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. (ì˜ˆ: "ì‚¬ìš©ìëŠ” ë§¤ìš´ ìŒì‹ì„ ì¢‹ì•„í•œë‹¤.")
    3. ë§Œì•½ ë‹¨ìˆœí•œ ì¸ì‚¬("ì•ˆë…•"), ê°íƒ„ì‚¬("ê·¸ë˜ìš”?")) ë“± ê¸°ì–µí•  ê°€ì¹˜ê°€ ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´, ì˜¤ì§ "ì—†ìŒ" ì´ë¼ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    4. ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ ë§ì€ ë§ë¶™ì´ì§€ ë§ê³ , ì˜¤ì§ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    """
    
    try:
        check_response = requests.post(
            OLLAMA_URL,
            json={
                "model": model,
                "prompt": extraction_prompt,
                "stream": False,
                "options": {"temperature": 2}
            },
            timeout=30
        )
        
        if check_response.status_code != 200:
            return

        extracted_info = check_response.json().get("response", "").strip()
        
        if "ì—†ìŒ" in extracted_info or len(extracted_info) < 2:
            # print(f"ë¬´ì‹œë¨ (ì •ë³´ ì—†ìŒ): {user_input}")
            return

        print(f"ğŸ’¡ ì¤‘ìš” ì •ë³´ ê°ì§€ë¨: {extracted_info}")

        current_memory = get_memory()
        
        merge_prompt = f"""
        ì—­í• : ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¥ê¸° ê¸°ì–µì„ ê´€ë¦¬í•˜ëŠ” ë¹„ì„œì…ë‹ˆë‹¤.
        
        [ê¸°ì¡´ ê¸°ì–µ]:
        {current_memory if current_memory else "(ì•„ì§ ê¸°ì–µ ì—†ìŒ)"}
        
        [ìƒˆë¡œìš´ ì •ë³´]:
        {extracted_info}
        
        ì§€ì‹œì‚¬í•­:
        1. [ìƒˆë¡œìš´ ì •ë³´]ë¥¼ [ê¸°ì¡´ ê¸°ì–µ]ì— ì¶”ê°€í•˜ì—¬ í•˜ë‚˜ë¡œ ì •ë¦¬ëœ ê¸°ì–µ ëª©ë¡ì„ ë§Œë“œì„¸ìš”.
        2. ë§Œì•½ [ìƒˆë¡œìš´ ì •ë³´]ê°€ [ê¸°ì¡´ ê¸°ì–µ]ê³¼ ì¶©ëŒí•œë‹¤ë©´, ìµœì‹  ì •ë³´ì¸ [ìƒˆë¡œìš´ ì •ë³´]ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ìš©ì„ ìˆ˜ì •í•˜ì„¸ìš”.
        3. ë‚´ìš©ì€ ë°˜ë“œì‹œ 'í•œêµ­ì–´'ë¡œ ì‘ì„±í•˜ê³ , ì½ê¸° ì‰½ê²Œ ê°œì¡°ì‹(bullet points)ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
        4. ì˜¤ì§ ì •ë¦¬ëœ ê¸°ì–µ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        """
        
        merge_response = requests.post(
            OLLAMA_URL,
            json={
                "model": model,
                "prompt": merge_prompt,
                "stream": False,
                "options": {"temperature": 0.1}
            },
            timeout=30
        )
        
        if merge_response.status_code == 200:
            new_memory = merge_response.json().get("response", "").strip()
            if new_memory:
                with open(MEMORY_FILE, "w", encoding="utf-8") as f:
                    f.write(new_memory)
                print("ê¸°ì–µ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

    except Exception as e:
        print(f"ê¸°ì–µ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.post("/api/chat")
def chat_stream():
    body = request.get_json(force=True, silent=True) or {}
    model = body.get("model", "gemma3:4b")
    messages = body.get("messages", [])
    options = body.get("options")

    user_memory = get_memory()

    system_content = "ë‹¹ì‹ ì€ ìœ ëŠ¥í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”."
    
    if user_memory:
        system_content += f"\n\n[ì‚¬ìš©ìì— ëŒ€í•´ ê¸°ì–µëœ ì •ë³´]:\n{user_memory}\n\nìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ì„¸ìš”. (ë‹¨, 'ë©”ëª¨ë¥¼ ì½ì—ˆë‹¤'ëŠ” í‹°ë¥¼ ë‚´ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì•„ëŠ” ì²™í•˜ì„¸ìš”.)"

    if messages and messages[0].get('role') == 'system':
        messages[0]['content'] = system_content + " " + messages[0]['content']
    else:
        messages.insert(0, {"role": "system", "content": system_content})

    try:
        upstream = requests.post(
            OLLAMA_CHAT_URL,
            json={
                "model": model,
                "messages": messages,
                "stream": True,
                **({"options": options} if options else {}),
            },
            stream=True,
            timeout=600,
        )
        upstream.raise_for_status()
    except requests.exceptions.RequestException as e:
        return jsonify({"error": str(e)}), 500

    last_user_msg = next((m['content'] for m in reversed(messages) if m['role'] == 'user'), None)
    
    if last_user_msg:
        threading.Thread(target=update_memory_task, args=(model, last_user_msg)).start()

    def generate():
        for line in upstream.iter_lines():
            if not line:
                continue
            yield line + b"\n"

    return Response(stream_with_context(generate()), mimetype="application/x-ndjson")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)